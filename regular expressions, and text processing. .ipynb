{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regular expressions, and text processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Regular Expressions \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**regular expression that extracts the urls out of this string.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To learn about pros/cons of data science, go to http://datascience.net.Alternatively, go to datascience.net/2020/ \n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"To learn about pros/cons of data science, go to http://datascience.net.\\\n",
    "Alternatively, go to datascience.net/2020/ \"\"\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://datascience.net\n",
      "datascience.net/2020/\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "url_reg=r'[(?:https?|ftp)+:\\/\\/]*[\\w/./-]*\\.[\\/a-z\\d\\-]+'\n",
    "match_url=re.findall(url_reg,text)\n",
    "if len(match_url)==0:\n",
    "    print(\"no url has found\")\n",
    "else :\n",
    "    for url in match_url:\n",
    "        print(url)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**regular expression that extracts all phone numbers and fax numbers from text.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"You can reach me at 054-434-4321, or my office at (03) 502 9571 or (050) 223 957.\\ \n",
    "Send me a fax at 03 502 7422. We finally made the sale for all 977 giraffes.\\\n",
    "They wanted 225 957 dollars for it.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "054-434-4321\n",
      "(03) 502 9571\n",
      "(050) 223 957\n",
      "03 502 7422\n"
     ]
    }
   ],
   "source": [
    "num_reg=r'[()\\d]+[\\-\\s]\\d+[\\-\\s]\\d+|\\d{10}|\\d{9}'\n",
    "match_nums=re.findall(num_reg,text)\n",
    "if len(match_nums)==0:\n",
    "    print(\"no number has found\")\n",
    "else :\n",
    "    for num in match_nums:\n",
    "        print(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**regular expression that extracts all opening html tags.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"This is <b>important</b> and <u>very</u><i>timely</i><br />. Was this <span> what you meant?</span>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<b>\n",
      "<u>\n",
      "<i>\n",
      "<br />\n",
      "<span>\n"
     ]
    }
   ],
   "source": [
    "tag_reg=r'<\\w*\\W*>'\n",
    "match_tags=re.findall(tag_reg,html)\n",
    "if len(match_nums)==0:\n",
    "    print(\"no tags has found\")\n",
    "else :\n",
    "    for tag in match_tags:\n",
    "        print(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**regular expression that extracts all the names of people**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Arnold Schwarzenegger was born in Austria. He and Sylvester Stalone used to run a restaurant\\\n",
    "with J. Edgar Hoover.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arnold Schwarzenegger\n",
      "Sylvester Stalone\n",
      "J. Edgar Hoover\n"
     ]
    }
   ],
   "source": [
    "name_reg=r'(?:[A-Z]\\.?\\w*\\s[A-Z]\\w*)(?:\\s[A-Z]\\w*)?'\n",
    "match_name=re.findall(name_reg,text)\n",
    "if len(match_name)==0:\n",
    "    print(\"no tags has found\")\n",
    "else :\n",
    "    for name in match_name:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**regular expression that extracts the text out of all html elements of class important.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lorem ipsum dolor <b>sit</b> amet, <b class=\"important\">consectetur adipiscing</b> elit,\\ \n",
      "sed do eiusmod <span id=\"note\">tempor incididunt ut</span> <div>labore <strong class=\"important\">et dolore magna</strong> aliqua.</div> Ut enim ad minim veniam, quis nostrud exercitation ullamco.\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Lorem ipsum dolor <b>sit</b> amet, <b class=\"important\">consectetur adipiscing</b> elit,\\ \n",
    "sed do eiusmod <span id=\"note\">tempor incididunt ut</span> <div>labore <strong class=\"important\">\\\n",
    "et dolore magna</strong> aliqua.</div> Ut enim ad minim veniam, quis nostrud exercitation ullamco.\"\"\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consectetur adipiscing\n",
      "et dolore magna\n"
     ]
    }
   ],
   "source": [
    "htmltext_reg=r'<\\w*\\sclass=\"important\">(.+?)<\\/\\w*>'\n",
    "match_text=re.findall(htmltext_reg,text)\n",
    "if len(match_text)==0:\n",
    "    print(\"no tags has found\")\n",
    "else :\n",
    "    for text in match_text:\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text processing: Comparing Netanyahu's and Gantz's Facebook Posts\n",
    "\n",
    "steps we follow are:\n",
    "1. Load the data set (you can use read_excel or other pandas functions)   \n",
    "+ Do preprocessing: (i.e punctuation removal, removal of english phrases, tokenization)\n",
    "+ Analyze word frequencies per candidate\n",
    "+ Find main differences between them (try to ignore function words)\n",
    "+ Try to look for other features (i.e text len, use of emoji's, etc), and look for differences\n",
    "+ Print a Wordcloud for each candidate with main words\n",
    "+ Evaluate the results, and write your insights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we need to import all useful lib\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import FreqDist\n",
    "from nltk.text import Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the dataset and Print the exact number of posts per candidate in the corpus.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Candidate</th>\n",
       "      <th>Amount of posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Benjamin Netanyahu</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Benny Gantz</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Candidate  Amount of posts\n",
       "0  Benjamin Netanyahu              186\n",
       "1         Benny Gantz              168"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the excel data into pandas df\n",
    "df=pd.read_excel('election_posts.xlsx')\n",
    "\n",
    "#extrect the posts freq for each candidate using nltk\n",
    "posts_amount=FreqDist(df['PageName'])#save frequency of posts into varible\n",
    "df_postamount=pd.DataFrame(list(posts_amount.items()), columns = [\"Candidate\",\"Amount of posts\"])#disply\n",
    "\n",
    "df_postamount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess the text data. In particular remove punctuations, remove english words, and tokenize the posts into token vectors (one vector for each post)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkn_Reg = RegexpTokenizer(r'[×-×ª]+')#regex to read only \n",
    "token_lst=[]#list for all token vectors\n",
    "\n",
    "def reg_token(post):#fun that will get a post and tokenize it\n",
    "    tmp=re.sub(r'[\\â€™\\\"\\×´]', \"\",post)#clean all typs of quote marks from the text\n",
    "    clean_post=tkn_Reg.tokenize(tmp)#tokenize the post \n",
    "    return clean_post#return the post clean and tokenize\n",
    "\n",
    "for index,row in df.iterrows():#loop over dataframe\n",
    "    tkn_Vec=(row.PageName,reg_token(row.Data))#create a vector(tuple) from candidate name and post\n",
    "    token_lst.append(tkn_Vec)#append each vector to list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze the total word frequencies, and frequencies of words per candidate( also from a relative perspective).** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Word  Frequency  Relative frequency(%)\n",
      "19       ××ª        210               5.857741\n",
      "33       ×©×œ        131               3.654114\n",
      "15       ×¢×œ        129               3.598326\n",
      "9        ×œ×        100               2.789400\n",
      "5     ×™×©×¨××œ         62               1.729428\n",
      "134     ×× ×™         57               1.589958\n",
      "101      ×¢×         53               1.478382\n",
      "95      ×’× ×¥         53               1.478382\n",
      "48       ×–×”         51               1.422594\n",
      "127      ×›×œ         49               1.366806\n",
      "330  ×”×××©×œ×”         48               1.338912\n",
      "10   × ×ª× ×™×”×•         47               1.311018\n",
      "183     ×¨××©         43               1.199442\n",
      "288   ×××©×œ×ª         37               1.032078\n",
      "340     ×›×“×™         36               1.004184\n",
      "29   ×”×œ×™×›×•×“         33               0.920502\n",
      "42       ××         33               0.920502\n",
      "14      ×”×•×         32               0.892608\n",
      "192      ×¨×§         30               0.836820\n",
      "41   ×œ×”×¦×‘×™×¢         26               0.725244\n",
      "\n",
      "       Word  Frequency  Relative frequency(%)\n",
      "53       ××ª        355               6.738800\n",
      "10    ×™×©×¨××œ        200               3.796507\n",
      "29       ×©×œ        198               3.758542\n",
      "3        ×¢×œ        194               3.682612\n",
      "149      ×œ×        161               3.056188\n",
      "60   × ×ª× ×™×”×•        122               2.315869\n",
      "235     ×× ×™        107               2.031131\n",
      "129      ×¢×         96               1.822323\n",
      "230      ×›×œ         74               1.404708\n",
      "86      ×”×•×         64               1.214882\n",
      "267   ×××©×œ×”         60               1.138952\n",
      "20      ×œ×‘×Ÿ         53               1.006074\n",
      "37       ×–×”         53               1.006074\n",
      "260      ×’×         53               1.006074\n",
      "19     ×›×—×•×œ         50               0.949127\n",
      "263     ×¨××©         45               0.854214\n",
      "12      ×”×›×œ         42               0.797267\n",
      "274     ×›×“×™         42               0.797267\n",
      "372   ××–×¨×—×™         41               0.778284\n",
      "161      ×œ×™         40               0.759301\n"
     ]
    }
   ],
   "source": [
    "total_words=[]#save all post words here\n",
    "gantz_words=[]#save all ganz words here\n",
    "bibi_words=[]#save all bibi words here\n",
    "\n",
    "for i in token_lst:#iterate on all vectors to extract\n",
    "    for word in i[1]:#iterate over token element on each tuple\n",
    "        total_words.append(word)#add words to var\n",
    "        if i[0]=='Benjamin Netanyahu':#check if its bibi post\n",
    "            bibi_words.append(word)#if yes append it to bibi var\n",
    "        else:#else do the same for ganz    \n",
    "            gantz_words.append(word)\n",
    "\n",
    "#find the frequancy of words \n",
    "total_freq=FreqDist(total_words)#total words freq\n",
    "gantz_freq=FreqDist(gantz_words)#ganz words freq\n",
    "bibi_freq=FreqDist(bibi_words)#bibi words freq\n",
    "\n",
    "bibi_anlz_df=pd.DataFrame(list(bibi_freq.items()), columns = [\"Word\",\"Frequency\"])#transfer into df\n",
    "gantz_anlz_df=pd.DataFrame(list(gantz_freq.items()), columns = [\"Word\",\"Frequency\"])#transfer into df\n",
    "\n",
    "#create a col with the relative freq of each word\n",
    "gantz_anlz_df['Relative frequency(%)']=(gantz_anlz_df['Frequency']/len(gantz_anlz_df))*100\n",
    "bibi_anlz_df['Relative frequency(%)']=(bibi_anlz_df['Frequency']/len(bibi_anlz_df))*100\n",
    "#sort the words, so the most useful words will be on top\n",
    "gantz_anlz_df = gantz_anlz_df.sort_values(by=['Relative frequency(%)'], ascending=False)\n",
    "bibi_anlz_df = bibi_anlz_df.sort_values(by=['Relative frequency(%)'], ascending=False)\n",
    "\n",
    "print(bibi_anlz_df[:20])\n",
    "print()\n",
    "print(gantz_anlz_df[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**find the main differences between the language and posts that each candidate uses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Word  Frequency  Relative frequency(%)\n",
      "19       ××ª        210               5.857741\n",
      "33       ×©×œ        131               3.654114\n",
      "15       ×¢×œ        129               3.598326\n",
      "9        ×œ×        100               2.789400\n",
      "5     ×™×©×¨××œ         62               1.729428\n",
      "134     ×× ×™         57               1.589958\n",
      "101      ×¢×         53               1.478382\n",
      "95      ×’× ×¥         53               1.478382\n",
      "48       ×–×”         51               1.422594\n",
      "127      ×›×œ         49               1.366806\n",
      "330  ×”×××©×œ×”         48               1.338912\n",
      "10   × ×ª× ×™×”×•         47               1.311018\n",
      "183     ×¨××©         43               1.199442\n",
      "288   ×××©×œ×ª         37               1.032078\n",
      "340     ×›×“×™         36               1.004184\n",
      "29   ×”×œ×™×›×•×“         33               0.920502\n",
      "42       ××         33               0.920502\n",
      "14      ×”×•×         32               0.892608\n",
      "192      ×¨×§         30               0.836820\n",
      "41   ×œ×”×¦×‘×™×¢         26               0.725244\n",
      "       Word  Frequency  Relative frequency(%)\n",
      "53       ××ª        355               6.738800\n",
      "10    ×™×©×¨××œ        200               3.796507\n",
      "29       ×©×œ        198               3.758542\n",
      "3        ×¢×œ        194               3.682612\n",
      "149      ×œ×        161               3.056188\n",
      "60   × ×ª× ×™×”×•        122               2.315869\n",
      "235     ×× ×™        107               2.031131\n",
      "129      ×¢×         96               1.822323\n",
      "230      ×›×œ         74               1.404708\n",
      "86      ×”×•×         64               1.214882\n",
      "267   ×××©×œ×”         60               1.138952\n",
      "20      ×œ×‘×Ÿ         53               1.006074\n",
      "37       ×–×”         53               1.006074\n",
      "260      ×’×         53               1.006074\n",
      "19     ×›×—×•×œ         50               0.949127\n",
      "263     ×¨××©         45               0.854214\n",
      "12      ×”×›×œ         42               0.797267\n",
      "274     ×›×“×™         42               0.797267\n",
      "372   ××–×¨×—×™         41               0.778284\n",
      "161      ×œ×™         40               0.759301\n"
     ]
    }
   ],
   "source": [
    "bibi_20=bibi_anlz_df[:20]\n",
    "gantz_20=gantz_anlz_df[:20]\n",
    "print(bibi_20)\n",
    "print(gantz_20)\n",
    "#bibi_t.collocations()\n",
    "#gantz_t.concordance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:** as we see from prev stats , bibi have more post and less words comparing to gantz its posts are shorter.\n",
    "words analyze:\n",
    "gantz talk about 'israel' 3.7% and bibi 1.7%\n",
    "bibi remaind gantz 1.47% and gantz remind bibi 2.37%\n",
    "gantz talk about himself more than bibi 2.03 compare 1.53 to the word '×× ×™\n",
    "bibi talk about votes and his faction more than gantz.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **interesting features that can show differences between the candidates (features such as post length, emoji's)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average len of bibi posts = 260.11\n",
      "Average len of gantz posts = 488.71\n",
      "\n",
      "  Emoji  Frequency\n",
      "0     !         82\n",
      "1     â¤          4\n",
      "2     ï¸         10\n",
      "3     ?         27\n",
      "4     ğŸ‡®         15\n",
      "5     ğŸ‡±         15\n",
      "6     ğŸ‘‡          3\n",
      "7     ğŸ‡º          1\n",
      "8     ğŸ‡¸          1\n",
      "9     â™¥          6\n",
      "  Emoji  Frequency\n",
      "0     !         31\n",
      "1     ?         19\n",
      "2     ï¿¼          1\n"
     ]
    }
   ],
   "source": [
    "bibi_len=[]\n",
    "gantz_len=[]\n",
    "#avg post len per candidate\n",
    "for i,row in df.iterrows():\n",
    "    if row.PageName=='Benjamin Netanyahu':\n",
    "        bibi_len.append(len(row.Data))\n",
    "    else:\n",
    "        gantz_len.append(len(row.Data))\n",
    "#average cacl        \n",
    "def Average(lst): \n",
    "    return sum(lst) / len(lst) \n",
    "\n",
    "bibi_average = Average(bibi_len)\n",
    "gantz_average=Average(gantz_len)\n",
    "print(\"Average len of bibi posts =\", round(bibi_average, 2)) \n",
    "print(\"Average len of gantz posts =\", round(gantz_average, 2))\n",
    "print()\n",
    "\n",
    "re_spcl = re.compile(r'[\\u263a-\\U0001f645\\!\\?]')#regex for emojis\n",
    "bibi_spcl=[]\n",
    "gantz_spcl=[]\n",
    "\n",
    "#iterate over df to create emoji list for each candidate\n",
    "for index,row in df.iterrows():#loop over dataframe\n",
    "    if row.PageName=='Benjamin Netanyahu':\n",
    "        bibi_spcl.append(re_spcl.findall(row.Data))\n",
    "    else:\n",
    "        gantz_spcl.append(re_spcl.findall(row.Data))\n",
    "\n",
    "#extend the lists\n",
    "bibi_res=[]\n",
    "[bibi_res.extend(el) for el in bibi_spcl]\n",
    "gantz_res=[]\n",
    "[gantz_res.extend(el) for el in gantz_spcl]\n",
    "#check freq of emojis\n",
    "bibimoji_freq=FreqDist(bibi_res)\n",
    "gantzmoji_freq=FreqDist(gantz_res)\n",
    "\n",
    "bibi_moji_df=pd.DataFrame(list(bibimoji_freq.items()), columns = [\"Emoji\",\"Frequency\"])#transfer into df\n",
    "gantz_moji_df=pd.DataFrame(list(gantzmoji_freq.items()), columns = [\"Emoji\",\"Frequency\"])\n",
    "print(bibi_moji_df)\n",
    "print(gantz_moji_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:** \n",
    "we analyzed 3 things:\n",
    "<br>\n",
    "1)according to last stats bibi have 186 post and gantz 168 post.\n",
    "<br>\n",
    "2)as we analyze avarege of posts length's bibi post are almost twice shorter than gantz posts.\n",
    "<br>\n",
    "3)after analyzing emoji's and !\\? , we see that bibi use much more emojis than gantz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**wordcloud for each candidate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "#import urllib\n",
    "import requests\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from bidi.algorithm import get_display\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.add(\"said\")\n",
    "\n",
    "#fixing and prepering text for word cloud\n",
    "textbibi = get_display(str(' '.join(bibi_words)))\n",
    "textgantz= get_display(str(' '.join(gantz_words)))\n",
    "\n",
    "#reading masks for web\n",
    "gantz_mask=np.array(Image.open(requests.get('https://cdn.imgbin.com/21/19/17/imgbin-jerusalem-flag-of-israel-magen-david-adom-judaism-fcexYfA8iT3EMgTg0UBWxmqqM.jpg', stream=True).raw))\n",
    "bibi_mask = np.array(Image.open(requests.get('http://clipart-library.com/images_k/star-of-david-silhouette/star-of-david-silhouette-7.png', stream=True).raw))\n",
    "\n",
    "#funcation that generate wordcloud \n",
    "def gen_wordcloud(words, mask):\n",
    "    word_cloud = WordCloud(font_path = 'C:\\Windows\\Fonts\\courbd.ttf',max_font_size=300,background_color=\"white\", max_words=3000, mask=mask, stopwords=stopwords).generate_from_text(words)\n",
    "    plt.figure(figsize=(9,7))\n",
    "    plt.imshow(word_cloud,cmap=plt.cm.gray,interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "gen_wordcloud(textbibi,bibi_mask)  \n",
    "gen_wordcloud(textgantz,gantz_mask) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:** to sum up , gantz talk more about israel and bibi with longer posts compare to bibi and about what his going to do diffrent compare to bibi\n",
    "bibi talk more about votes and also about himself (1.37% to the word \"× ×ª× ×™×”×•\") with shorter posts (his talk more '×ª×›×œ×¡')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
